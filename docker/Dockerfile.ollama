FROM ubuntu:22.04

# Install curl and keep it installed
RUN apt-get update && apt-get install -y \
    curl \
    && rm -rf /var/lib/apt/lists/*

# Install Ollama from website
RUN curl https://ollama.ai/install.sh | sh

# Create startup script with server readiness check
RUN echo '#!/bin/bash\n\
\n\
# Start Ollama server in the background\n\
ollama serve &\n\
\n\
# Wait for server to be ready\n\
max_attempts=30\n\
attempt=1\n\
echo "Waiting for Ollama server to start..."\n\
while [ $attempt -le $max_attempts ]; do\n\
    if curl -s http://localhost:11434/api/version >/dev/null; then\n\
        echo "Ollama server is ready"\n\
        break\n\
    fi\n\
    echo "Attempt $attempt of $max_attempts - waiting for Ollama server..."\n\
    sleep 2\n\
    attempt=$((attempt + 1))\n\
done\n\
\n\
if [ $attempt -gt $max_attempts ]; then\n\
    echo "Failed to start Ollama server after $max_attempts attempts"\n\
    exit 1\n\
fi\n\
\n\
# Now pull the models\n\
echo "Pulling models..."\n\
ollama pull llama3.2:3b\n\
ollama pull gemma2:2b\n\
ollama pull qwen2.5:3b\n\
\n\
# Keep the container running\n\
wait' > /start.sh && chmod +x /start.sh

EXPOSE 11434

CMD ["/start.sh"]